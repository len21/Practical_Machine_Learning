This section focuses on perform residuals analysis based on the regression model selected above. We will look at regression diagnostics for the model and see if we see any outliers in the data set.
```{rd ,echo=TRUE}
par(mfrow = c(2, 2))
plot(fit2)
```
Based on the plots above I can make the following observations:
* The points on the Residuals vs. Fitted plot seem to be randomly scattered and verify the independence condition.
* The Normal Q-Q plot consists of the points which mostly fall on the line indicating that the residuals are normally distributed.
* The Scale-Location plot consists of points scattered in a constant band pattern, indicating constant variance.
* There are some distinct points of interest (outliers or leverage points) in the top right of the plots.
## Conclusion
Although in this data set on average manual vehicles achieve a fuel efficiency of 7.2 miles per gallon more than automatic vehicles, transmission type is not a particularly good predictor of fuel efficiency. We were able to identify that the number of cylinders and the weight of the automobile are good predictors of fuel efficiency, achieving an adjusted R squared of 0.82. If we add transmission type to this model, then the difference in fuel effiency for a manual transmission is much smaller, just 0.18 miles per gallon for a vehicle with the same weight and number of cylinders. Therefore we conclude that number of cylinders and weight are good predictors of fuel efficiency, but transmission type is not.
## Appendix
Below is the code the was used to create the data and diagrams that are not already included in the above document.
```{r ,eval = TRUE}
# review the data
str(mtcars)
# see the a couple lines
```
```{r boxplot,echo=FALSE}
boxplot(mpg~am, data = mtcars,
col = c("dark grey", "light grey"),
xlab = "Transmission",
ylab = "Miles per Gallon",
main = "Fig 1 - MPG by Transmission Type")
scatterplotMatrix(mtcars[,1:11],
diagonal="boxplot",
main="Fig 2- Scatterplot",
smooth=FALSE)
```
t.test(mpg ~ am, data = mtcars)
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)
summary(fit)$coef[3, 1]
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)
fit1 <- lm(mpg ~ factor(cyl) , data = mtcars)
summary(fit1)
summary(fit1)$coef[3]
summary(fit)$coef[3]
fit2 <- lm(mpg ~ factor(cyl)*wt, mtcars)
summary(fit2)
summary(fit2)
summary(fit)
compare <- anova(fit, fit2)
compare$Pr
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
summary(fit2)
fit2 <- lm(mpg ~ factor(cyl)*wt, mtcars)
summary(fit2)
compare <- anova(fit, fit2)
compare$Pr
compare
compare$Pr
compare
compare <- anova(fit, fit2)
result <- anova(fit, fit3, test="Chi")
result <- anova(fit, fit2, test="Chi")
result
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
hatvalues(fit5)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit6 <- lm(y ~ x)
summary(fit6)
dfbetas(fit5)[5, 2]
hatvalues(fit6)
dfbetas(fit6)[5, 2]
library(knitr)
library(ggplot2)
library(datasets)
library(car)
data(mtcars)
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
mtcars$cyl <- factor(mtcars$cyl)
mean(mtcars$am)
fit <- aov(mpg ~ ., data = mtcars)
summary(fit)
fit1 <- lm(mpg ~ cyl + disp + wt + am, data = mtcars)
summary(fit1)
fit1 <- lm(mpg ~ cyl + disp + wt + am, data = mtcars)
fit2 <- lm(mpg ~ cyl + wt + am, data = mtcars)
fit3 <- lm(mpg ~ cyl + wt, data = mtcars)
fit4 <- lm(mpg ~ am, data = mtcars)
fit1
summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)
fit1 <- lm(mpg ~ cyl + disp + wt + am, data = mtcars)
summary(fit1)
fit2 <- lm(mpg ~ cyl + wt + am, data = mtcars)
summary(fit2)
fit3 <- lm(mpg ~ wt + am, data = mtcars)
summary(fit3)
fit1 <- lm(mpg ~ cyl + disp + wt + am, data = mtcars)
summary(fit1)
fit2 <- lm(mpg ~ cyl + wt + am, data = mtcars)
summary(fit2)
fit <- aov(mpg ~ ., data = mtcars)
summary(fit)
fit <- aov(mpg ~ ., data = mtcars)
summary(fit)
fit1 <- lm(mpg ~ am, data = mtcars)
summary(fit1)
t.test(mpg ~ am, data = mtcars)
fit2 <- lm(mpg ~ disp + wt + am, data = mtcars)
summary(fit2)
fit3 <- lm(mpg ~ wt + am, data = mtcars)
summary(fit3)
anova(fit1, fit2)
anova(fit1, fit3)
anova(fit1, fit2, fit3)
## testing for best fit of model
par(mfrow = c(2, 2))
plot(fit1)
par(mfrow = c(2, 2))
plot(fit1)
par(mfrow = c(2, 2), mar=c(1,1,1,1))
plot(fit1)
plot(fit3)
xmodels <- anova(fit1, fit2, fit3)
summary(xmodels)$coef
?anova
coef(anova(fit1, fit2, fit3))
summary(anova(fit1, fit2, fit3))$coef
summary(fit3)$coef
summary(fit2)$coef
summary(fit1)$coef
cofint(fit3)
confint(fit3)
confint(fit2)
confint(fit1)
coef(fit1)
coef(fit3)
coef(fit2)
summary(fit2)$coefficients
fit2 <- lm(mpg ~  wt + cyl + am, data = mtcars)
summary(fit2)
fit3 <- lm(mpg ~ wt + am, data = mtcars)
summary(fit3)
fit2 <- lm(mpg ~ wt + cyl + am, data = mtcars)
fit3 <- lm(mpg ~ wt + am, data = mtcars)
anova(fit1, fit2, fit3)
anova(fit1, fit3)
anova(fit1, fit2)
anova(fit1, fit2, fit3)
fit2 <- lm(mpg ~  wt + cyl + am + wt:am, data = mtcars)
summary(fit2)
fit3 <- lm(mpg ~ wt + am + wt:am, data = mtcars)
summary(fit3)
par(mfrow = c(2, 2), mar=c(1,1,1,1))
plot(fit2)
fit1 <- lm(mpg ~ am, data = mtcars)
summary(fit1)
t.test(mpg ~ am, data = mtcars)
plot(fit2, main= 'Fig 3.)
''
)
''
plot(fit2, main= 'Fig 3.')
plot(fit2, title= 'Fig 3.')
library(MASS)
head(shuttle)
dim(shuttle)
head(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
head(shuttle)
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2] # 0.9686888
head(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
exp(summary(fit)$coef)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
exp(summary(fit)$coef)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ wind, family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind + as.factor(magn), family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind + as.factor(magn), family='binomial', data=shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind, family='binomial', data=shuttle)
exp(fit$coeff)
summary(fit)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2]
fit <- glm(newUse ~ wind - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2] # 0.9686888
fit <- glm(newUse ~ wind + magn - 1, family="binomial", data=shuttle)
summary(fit)$coef
exp(coef(fit))
odds <- exp(cbind(OddsRatio = coef(fit), confint(fit)))
odds[1] / odds[2]
fit <- glm(use ~ wind + magn, family='binomial', data=shuttle)
exp(fit$coeff)
1.4384/1.4852
fit<-glm(newUse ~ wind + magn) - 1, family = binomial, data = shuttle2)
exp(coef(fit))
exp(cbind(OddsRatio = coef(fit), confint(fit)))
1.4384/1.4852
fit<-glm(newUse ~ wind, family = binomial, data = shuttle)
summary(fit)$coef
fit2<-glm(1 - newUse ~ wind, family = binomial, data = shuttle)
summary(fit2)$coef
dim(InsectSprays)
head(InsectSprays)
fitb <- glm(count ~ spray - 1, data=InsectSprays, family="poisson")
summary(fitb)$coef
rate <- exp(coef(fitb))
str(InsectSprays)
rate <- exp(coef(fitb))
rate
coef(fitb)
coef(fitb)
str(InsectSprays)
fitb2<-glm(count~spray-1,data=InsectSprays,family=poisson)
summary(fitb2)$coef
exp(coef(fitb2))
14.500  / 15.333
fitb<-glm(count ~ spray, family = poisson, data=InsectSprays, offset = log(count + 1))
summary(fitb)$coef
fitb2<-glm(count ~ factor(spray), family = poisson,data=InsectSprays,offset = log(10)+log(count+1))
summary(fitb2)$coef
fit <- glm(count ~ spray + offset(log(count+1)), family="poisson", data=InsectSprays)
fit2 <- glm(count ~ spray + offset(log(10)+log(count+1)),family="poisson", data=InsectSprays)
summary(fit)$coef
summary(fit2)$coef
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knotPoint <- c(0)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
Problem 5.
# Suppose that we have created a machine learning algorithm that predicts whether a link will be
# clicked with 99% sensitivity and 99% specificity. The rate the link is clicked is 1/1000 of
# visits to a website. If we predict the link will be clicked on a specific visit,
# what is the probability it will actually be clicked?
# 100,000 visits => 100 clicks
# 99% = sensitivity = TP/(TP+FN) = 99/(99+1) = 99/100
# 99% specificity =TN/(TN+FP) = 98901/(98901+999) = 98901/99900
# P(actually clicked|clicked) = TP/(TP+FP) = 99/(99+999) = 9%
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
library(caret)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
library(ggplot2)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
training
?createDataPartition
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training)
str(training)
hist(training$Superplasticizer,main="",xlab="Superplasticizer")
qplot(CompressiveStrength, Cement, data=concrete)
index<-colnames(concrete[,c(1,2,3,4,5,6,7)])
featurePlot(x=training[,index], y=training$CompressiveStrength, plot="pairs")
summary(training)
summary(training)
index
featurePlot(x=training[,1-8], y=training$CompressiveStrength, plot="pairs")
featurePlot(x=training[,1-8], y=training$CompressiveStrength, plot="pairs")
qplot(CompressiveStrength, Cement, data=concrete)
hist(training$Superplasticizer,main="",xlab="Superplasticizer")
summary(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featurePlot(x=training[,1-8], y=training$CompressiveStrength, plot="pairs")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
summary(training)
str(training)
ols_IL <- training[,grep('^IL', x = names(training) )]
Cols_IL <- training[,grep('^IL', x = names(training) )]
preObj <- preProcess(training[,Cols_IL],method=c("center","scale"))
preObj <- preProcess(Cols_IL, method=c("center","scale"))
preObj
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = training$Superplasticizer)) + geom_histogram() + theme_bw()
library(ggplot2)
ggplot(data = training, aes(x = training$Superplasticizer))
qp <- ggplot(data = training, aes(x = training$Superplasticizer))
qp
qplot(data = training, aes(x = training$Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = training$Superplasticizer))
Cols_IL <- grep("^IL", names(training), value = TRUE)
Cols_IL <- grep("^IL", names(training), value = TRUE)
preObj <- preProcess(training[, Cols_IL], method = "pca", thresh = 0.9)
preObj
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
Cols_IL <- grep("^IL", names(training), value = TRUE)
predictors_IL <- predictors[, IL_str]
adData
View(adData)
View(adData)
predictors_IL <- adData[, IL_str]
predictors_IL <- adData[, Cols_IL]
df2 <- data.frame(diagnosis, predictors_IL)
predictors_IL <- training[, Cols_IL]
df2 <- data.frame(diagnosis, predictors_IL)
# Clean up workspace
rm(list=ls())
# load packages needed
library(ggplot2)
library(caret)
# set working directory to the location where the data is
setwd("/Users/moiralennox/GitHub/Practical_Machine_Learning")
# load data make all blanks, null = NA
dftraining <- read.csv("./data/pml-training.csv", na.string=c("","NA","NULL"), row.names = 1)
#str(dftraining)
dftesting <- read.csv("./data/pml-testing.csv", na.string=c("","NA","NULL"), row.names = 1)
#str(dftesting)
## Clean-up data
dfcleanData <- dftraining[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanData <- dfcleanData[,-c(1:6)] #First 6 Columns Of The dataset are removed
prComp <- prcomp(dfcleanData)
prComp <- prcomp(dfcleanData[,-c(53)])
plot(prComp$x[,1],prComp$x[,2])
.pardefault <- par(no.readonly = T)
plot(prComp$x[,1],prComp$x[,2])
nearZeroVar(dfcleanData)
prComp <- prcomp(dfcleanData[,-c(53)])
prComp
prComp$rotation
preProc <- preProcess(dfcleanData[,-53]),method="pca")
dfcleanData <- dftraining[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanData <- dfcleanData[,-c(1:6)] #First 6 Columns Of The dataset are removed
# Remove near zero covariates
dfcleanData <- nearZeroVar(dfcleanData)
dfcleanData <- dftraining[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanData <- dfcleanData[,-c(1:6)] #First 6 Columns Of The dataset are removed
dfcleanData <- nearZeroVar(dfcleanData)
dfcleanData1 <- nearZeroVar(dfcleanData, saveMetrics=TRUE)
NZV <- nearZeroVar(dfcleanData, saveMetrics=TRUE)
dfcleanData1 <- dfcleanData[!myNZVvars]
dfcleanData1 <- dfcleanData[!NZV]
dfcleanData <- dftraining[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanData <- dfcleanData[,-c(1:6)] #First 6 Columns Of The dataset are removed
NZV <- nearZeroVar(dfcleanData, saveMetrics=TRUE)
NZV
dfcleanData <- dftraining[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanData <- dfcleanData[,-c(1:6)] #First 6 Columns Of The dataset are removed
dim(dfcleanData)
NZV <- nearZeroVar(dfcleanData, saveMetrics=TRUE)
filteredDescr <- dfcleanData[, -NZV]
NZV
NZV <- nearZeroVar(dfcleanData)
NZV
filteredDescr <- dfcleanData[, -NZV]
dfcleanData <- dftraining[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanData <- dfcleanData[,-c(1:6)] #First 6 Columns Of The dataset are removed
dim(dfcleanData)
NZV <- nearZeroVar(dfcleanData, saveMetrics=TRUE)
NZV
NZV <- nearZeroVar(dfcleanData, saveMetrics=TRUE)
NZV
?createDataPartition
set.seed(1234)
index_train <- createDataPartition(y=dfcleanData$classe, p=0.60, list=FALSE)
data_train <- pml_train[index_train,feature_index]
data_xval <- pml_train[-index_train,feature_index]
dim(data_train)
dim(data_xval)
data_train <- dfcleanData[index_train,feature_index]
data_xval <- dfcleanData[-index_train,feature_index]
dim(data_train)
dim(data_xval)
data_train <- dfcleanData[index_train]
data_xval <- dfcleanData[-index_train]
dim(data_train)
index_train
index_train <- createDataPartition(y=dfcleanData$classe, p=0.60, list=FALSE)
data_train <- dfcleanData[index_train,]
data_xval <- dfcleanData[-index_train,]
dim(data_train)
dim(data_xval)
plot(data_train$classe, xlab="classe levels", ylab="Frequency")
modFit<- train(classe ~ ., method = "lm",data=data_train)
modFit <- train(classe ~ .,data=data_train,method="lm")
modFit <- train(classe ~ .,method="rpart",data=data_train)
install.packages('rpart')
install.packages("rpart")
install.packages('rpart')
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
library(rpart)
modFit <- train(classe ~ .,method="rpart",data=data_train)
library(ggplot2)
library(caret)
modFit <- train(classe ~ .,method="rpart",data=data_train)
print(modFit$finalModel)
confusionMatrix(data_xval$classe,predict(modFit,data_xval))
modFit <- train(classe~.,data=data_train,method="rf",trControl = trainControl(method="cv",number=4,allowParallel=TRUE))
install.packages("randomForest", dependencies = TRUE)
install.packages("randomForest", dependencies = TRUE)
library(ggplot2)
library(caret)
#install.packages('rpart')
library(rpart)
#install.packages("randomForest", dependencies = TRUE)
library(randomForest)
modFit <- train(classe~.,data=data_train,method="rf",trControl = trainControl(method="cv",number=4,allowParallel=TRUE))
confusionMatrix(data_xval$classe,predict(modFit,data_xval))
dfcleanDataTest <- dftesting[,which(as.numeric(colSums(is.na(dftraining)))==0)]
dfcleanDataTest <- dfcleanDataTest[,-c(1:6)] #First 6 Columns Of The dataset are removed
predict_answers <- predict(modFit,dfcleanDataTest)
predict_answers
x <- as.data.frame(predict_answers)
head(x)
print(predict_answers)
answers = rep("A", 20)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
setwd("/Users/moiralennox/GitHub/Practical_Machine_Learning/PredictFiles")
pml_write_files(answers)
print(predict_answers)
n = length(predict_answers)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(answers[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
pml_write_files(answers)
setwd("/Users/moiralennox/GitHub/Practical_Machine_Learning/PredictFiles")
n = length(predict_answers)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(answers[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
n = length(predict_answers)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(predict_answers[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
pml_write_files(answers)
predict_answers
setwd("/Users/moiralennox/GitHub/Practical_Machine_Learning/PredictFiles")
n = length(predict_answers)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(predict_answers[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
#then create a folder where you want the files to be written. Set that to be your working directory and run:
pml_write_files(predict_answers)
